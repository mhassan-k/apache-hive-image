# apache-hive-image
This tutorial uses Docker containers to spin up Apache Hive. Before we jump right into it, here is a quick overview of some of the critical components in this cluster.
## Apache Hive:
Apache Hive is a distributed, fault-tolerant data warehouse system that enables analytics of large datasets residing in distributed storage using SQL.
## Docker:
Docker is an open-source technology to package an application and all its dependencies into a container.
## NameNode:
The NameNode is at the core of the Hadoop cluster. It keeps the directory tree of all files in the file system, and tracks where the file data is actually kept in the cluster.
## DataNode:
The DataNode on the other hand stores the actual file data. Files are replicated across multiple DataNodes in a cluster for reliability.
Specifically, if were to think in terms of Hive, the data stored on the Hive tables is spread across the DataNodes within the cluster. NameNode, on the other hand is the one keeping track of these blocks of data actually stored on the DataNodes.
We are using a single DataNode in this tutorial for the sake of simplicity.
## Hive Metastore:
Hive uses a relational database to store the metadata (e.g. schema and location) of all its tables. The default database is Derby, but we will be using PostgreSQL in this tutorial.
The key benefit of using a relational database over HDFS is low latency and improved performance.
## Volumes:
Volumes are the preferred mechanism for persisting data generated by and used by Docker containers.
Volumes also allow us to persist the container state between subsequent docker runs.


## how to use it : 
1- clone the repo 
